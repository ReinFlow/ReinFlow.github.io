<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ReinFlow Experiments</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        });
    </script>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0 auto;
            max-width: 900px;
            padding: 20px;
            background-color: #f9f9f9;
            color: #333;
        }
        h1 {
            font-size: 2.5em;
            color: #2c3e50;
            text-align: center;
            margin-bottom: 20px;
        }
        h2 {
            font-size: 1.8em;
            color: #34495e;
            margin-top: 40px;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
        }
        h3 {
            font-size: 1.4em;
            color: #34495e;
            margin-top: 30px;
        }
        p {
            font-size: 1.1em;
            margin: 10px 0;
        }
        .figure-container {
            margin: 20px 0;
            background: #fff;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
            text-align: center;
        }
        .subfigure-row {
            display: flex;
            justify-content: space-between;
            flex-wrap: wrap;
            margin-bottom: 10px;
        }
        .subfigure {
            flex: 0 0 24%;
            text-align: center;
            margin-bottom: 10px;
        }
        .subfigure-3 {
            flex: 0 0 32%;
            text-align: center;
            margin-bottom: 10px;
        }
        .subfigure-2 {
            flex: 0 0 49%;
            text-align: center;
            margin-bottom: 10px;
        }
        img {
            width: 600px;
            height: 400px;
            object-fit: contain;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 5px;
            display: block;
            margin: 0 auto;
        }
        .subfigure img {
            width: 100%;
            height: 200px;
            object-fit: contain;
        }
        .subfigure-3 img {
            width: 100%;
            height: 200px;
            object-fit: contain;
        }
        .subfigure-2 img {
            width: 100%;
            height: 250px;
            object-fit: contain;
        }
        .legend img {
            width: 600px;
            height: 100px; /* Reduced height for legend */
            object-fit: contain;
        }
        img[alt="Legend44"] {
            height: 40%;
            width: 64% ; /* Maintain aspect ratio auto */
        }
        img[alt="Legend4"] {
            height: 50%;
            width: 80% ; /* Maintain aspect ratio auto */
        }
        img[alt="Legend3"] {
            height: 20%;
            width: 60% ; /* Maintain aspect ratio auto */
        }
        .caption {
            font-size: 0.95em;
            color: #555;
            text-align: left;
            margin-top: 10px;
        }
        .subfigure-caption {
            font-size: 0.9em;
            color: #555;
            margin-top: 5px;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        /*Remove margins around figures*/
        .subfigure-row {
            margin-bottom: 2px !important; /* Was 10px or more */
        }
        .figure-container.legend {
            margin-top: 0px !important;
            margin-bottom: 0px !important;
            background: transparent !important;
            box-shadow: none !important;
            padding: 0 !important;
        }

        .figure-container.legend img[alt="Legend3"],
        .figure-container.legend img[alt="Legend4"] {
            margin: 0 !important;
            padding: 0 !important;
            border: none !important;
            border-radius: 0 !important;
            box-shadow: none !important;
            background: transparent !important;
            display: block;
            width: 100%;   /* Or adjust as needed, e.g., 60% */
            height: 40px;  /* Aggressively reduce height */
            object-fit: contain;
        }
        .figure-container.legend img[alt="Legend44"] {
            margin: 0 !important;
            padding: 0 !important;
            border: none !important;
            border-radius: 0 !important;
            box-shadow: none !important;
            background: transparent !important;
            display: block;
            width: 1000%;   /* Or adjust as needed, e.g., 60% */
            height: 25px;  /* Aggressively reduce height */
            object-fit: contain;
        }
    </style>
</head>
<body>
    <h1>ReinFlow: Experiments</h1>

    <!-- Notes from tcolorbox -->
    <!-- 1. Downsample FQL -->
    <!-- 2. Add BC baselines to all figures -->
    <!-- 3. Add pre-training model comparison plot in frequency view -->
    <!-- Some important experiments left untouched: -->
    <!-- 2. Using direct evaluation method with Hutchinson's trace estimators -->
    <!-- Also should mention that at very few steps there is a large discretization error when using Euler integral to the equations derived from SDE -->
    <!-- Say that we care more about simulation time than sample as this is not a real-world RL problem -->
    <!-- For neps=16 square datascale just say fine-tuning failed and we use pre-trained policy's success rate -->
    <!-- For transport-image task we test the wall-clock-time with osmesa as rendering with egl causes OOM problem -->
    <!-- Really need to stress that we share the same batch size with DPPO as a fair comparison -->
    <!-- The wall clock time comparison is unfair for FQL as the batch size is a lot smaller -->
    <!-- Maybe FQL experiments need to be re-run to align with 500 steps of rollouts -->

    <section id="experiments">
        <h2>Experiments</h2>
        <p>We run simulated robot learning experiments to test the effectiveness and flexibility of our method, ReinFlow. We aim to adopt ReinFlow to significantly enhance the success rate of pre-trained flow matching policies trained on mediocre expert data. We also managed to fine-tune at very few or even one denoising step for Rectified Flow (indicated by "ReinFlow-R") and Shortcut Models (indicated by "ReinFlow-S"), where fine-tuning the two types of pre-trained models shares the same training hyperparameters.</p>
        <p>We adopt a PPO-based implementation of our algorithm due to its stability. While alternative implementations may offer higher sample efficiency, we prioritize wall time efficiency in simulated environments over sample cost. Exploring more sample-efficient RL algorithms for ReinFlow, especially in real-world scenarios where data collection is expensive, is an interesting direction for future work.</p>
        <p>We compare ReinFlow against various RL methods for fine-tuning diffusion and flow-based policies, particularly DPPO and FQL. DPPO is a strong online RL algorithm for diffusion policies, developed under a bi-level MDP formulation. FQL represents state-of-the-art offline RL for flow-matching policies and applies to offline-to-online fine-tuning. Comparisons to other baselines are provided in the Appendix.</p>
    </section>

    <section id="env-setup">
        <h3>Environment Setup and Data Curation</h3>
        <p>We compare the algorithms in locomotion tasks and four manipulation tasks. In locomotion tasks, the agent receives state input and dense reward, and it resembles how we train legged robots via sim2real RL in modern applications. We also consider more challenging and realistic manipulation tasks, where agents receive pixel and/or state inputs with sparse rewards.</p>
        <h4>OpenAI Gym</h4>
        <p>We fine-tuned flow matching policies with ReinFlow in "Hopper-v2", "Walker2d-v2", "Ant-v2", and "Humanoid-v3", where these tasks are listed in ascending difficulty. "Ant-v2" and "Humanoid-v3" environments involve high-dimensional state inputs and challenge continuous control problems. The expert data are demonstrations collected from the D4RL dataset, except for the "Humanoid-v3" task, where the data is sampled from our own pre-trained SAC agent, at medium or medium-expert level.</p>
        <h4>Franka Kitchen</h4>
        <p>In Franka Kitchen, a 9-DoF Franka robot learns long-horizon multitask planning by sequentially completing four state-based manipulation tasks. We pre-train the models with human-teleoperated data with complete, mixed, or partial demonstrations of the four tasks.</p>
        <h4>Robomimic</h4>
        <p>We train agents in visual manipulation tasks in Robomimic, including a simple pick-and-place task "PickPlaceCan" (Can), assembly task "NutAssemblySquare" (Square), and bi-manual manipulation task "TwoArmTransport" (Transport). Data are collected via human teleportation. The Robomimic datasets are further processed like DPPO, which contains data of lower quantity and/or quality than proficient human demonstrations.</p>
    </section>

    <section id="performance-evaluation">
        <h3>Performance Evaluation</h3>
        <p>ReinFlow demonstrates strong training stability with a significant reward or success rate net increase in all tasks. In Gym and Franka Kitchen benchmarks, it achieves the best overall efficiency and performance improvement.</p>
        <div class="figure-container">
        <div class="subfigure-row">
            <div class="subfigure">
                    <img src="figs/jpgs/gym-state_hopper-d4rl_AverageEpisodeReward_wallclock-page1.jpg" alt="Hopper-v2 Wall Time">
                    <div class="subfigure-caption">(A) Hopper-v2</div>
                </div>
                <div class="subfigure">
                    <img src="figs/jpgs/gym-state_walker-d4rl_AverageEpisodeReward_wallclock-page1.jpg" alt="Walker-v2 Wall Time">
                    <div class="subfigure-caption">(B) Walker-v2</div>
                </div>
                <div class="subfigure">
                    <img src="figs/jpgs/gym-state_ant-d4rl_AverageEpisodeReward_wallclock-page1.jpg" alt="Ant-v2 Wall Time">
                    <div class="subfigure-caption">(C) Ant-v2</div>
                </div>
                <div class="subfigure">
                    <img src="figs/jpgs/gym-state_humanoid-d4rl_AverageEpisodeReward_wallclock-page1.jpg" alt="Humanoid-v3 Wall Time">
                    <div class="subfigure-caption">(D) Humanoid-v3</div>
                </div>
            </div>
            <div class="figure-container legend">
                <img src="figs/jpgs/gym-state_hopper-d4rl_AverageEpisodeReward_legend_crop-page1.jpg" alt="Legend44">
            </div>
            <div class="caption">
                Wall time efficiency results of state-based locomotion tasks in OpenAI Gym. Dashed lines indicate the behavior cloning level.
            </div>
        </div>
        <div class="figure-container">
            <div class="subfigure-row">
                <div class="subfigure">
                    <img src="figs/jpgs/kitchen_kitchen-complete-v0_TaskCompletionRate_wallclock-page1.jpg" alt="Kitchen-complete Wall Time">
                    <div class="subfigure-caption">(A) Kitchen-complete</div>
                </div>
                <div class="subfigure">
                    <img src="figs/jpgs/kitchen_kitchen-complete-v0_TaskCompletionRate-page1.jpg" alt="Kitchen-complete Sample">
                    <div class="subfigure-caption">(B) Kitchen-complete</div>
                </div>
                <div class="subfigure">
                    <img src="figs/jpgs/kitchen_kitchen-mixed-v0_TaskCompletionRate-page1.jpg" alt="Kitchen-mixed Sample">
                    <div class="subfigure-caption">(C) Kitchen-mixed</div>
                </div>
                <div class="subfigure">
                    <img src="figs/jpgs/kitchen_kitchen-partial-v0_TaskCompletionRate-page1.jpg" alt="Kitchen-partial Sample">
                    <div class="subfigure-caption">(D) Kitchen-partial</div>
                </div>
            </div>
            <div class="figure-container legend">
                <img src="figs/jpgs/kitchen_kitchen-complete-v0_TaskCompletionRate_legend_crop-page1.jpg" alt="Legend3">
            </div>
            <div class="caption">
                Task completion rates of state-input manipulation tasks in Franka Kitchen. (A) and (B) show the completion rates against the wall time and sample cost. We only show the sample cost plots for agents trained on mixed and partial datasets for brevity.
            </div>
        </div>
        <p>Across three Robomimic visual manipulation tasks, ReinFlow-S and ReinFlow-R improve the success rate of the pre-trained policy by an average of 40.09%. ReinFlow achieves success rates comparable to DPPO, requiring significantly fewer fine-tuning steps and less wall-clock time. Notably, it uses fewer denoising steps: just <i>one</i> in Can and Square, and a four-step flow in Transport, compared to the five-step DDIM used by DPPO.</p>
        <div class="figure-container">
            <div class="subfigure-row">
                <div class="subfigure-3">
                    <img src="figs/jpgs/robomimic-img_can-img_SuccessRate-page1.jpg" alt="Can Success Rate">
                    <div class="subfigure-caption">(A) Can</div>
                </div>
                <div class="subfigure-3">
                    <img src="figs/jpgs/robomimic-img_square-img_SuccessRate-page1.jpg" alt="Square Success Rate">
                    <div class="subfigure-caption">(B) Square</div>
                </div>
                <div class="subfigure-3">
                    <img src="figs/jpgs/robomimic-img_transport-img_SuccessRate-page1.jpg" alt="Transport Success Rate">
                    <div class="subfigure-caption">(C) Transport</div>
                </div>
            </div>
            <div class="figure-container legend">
                <img src="figs/jpgs/robomimic-img_can-img_SuccessRate_legend_crop-page1.jpg" alt="Legend4">
            </div>
            <div class="caption">
                Success rates in visual manipulation tasks in Robomimic.
            </div>
        </div>
    </section>

    <section id="design-choice">
        <h3>The Design Choice and Key Factors Affecting ReinFlow</h3>
        <p>This section analyzes how the pre-trained model and denoising steps affect our algorithm. We also study the effects of the noise level, the type, and the intensity of regularization.</p>
        <h4>Scaling</h4>
        <p>We fine-tune flow matching policies trained on datasets with different numbers of episodes and test the performance of pre-trained and fine-tuned models at different denoising steps. The figure below reveals that scaling inference steps and/or pre-training data quantity does not consistently improve the reward, which is consistent with the findings in data scaling laws. However, ReinFlow consistently improves the success rate or reward over the pre-trained policies regardless of the pre-training scale.</p>
        <div class="figure-container">
            <div class="subfigure-row">
                <div class="subfigure-3">
                    <img src="figs/jpgs/avg_episode_reward_3d_surface-page1.jpg" alt="ReFlow Policy in Hopper-v2">
                    <div class="subfigure-caption">(A) ReFlow Policy in Hopper-v2</div>
                </div>
                <div class="subfigure-3">
                    <img src="figs/jpgs/datascale_shortcut_square_success_rate_2d-page1.jpg" alt="Shortcut Policy in Square">
                    <div class="subfigure-caption">(B) Shortcut Policy in Square</div>
                </div>
                <div class="subfigure-3">
                    <img src="figs/jpgs/robomimic-img_square-img-logitbeta_SuccessRate-page1.jpg" alt="ReFlow Policy in Square">
                    <div class="subfigure-caption">(C) ReFlow Policy in Square</div>
                </div>
            </div>
            <div class="caption">
                RL provides another way of scaling apart from increasing pre-training data and inference consumption, which has a plateauing effect. The improvement is invariant of flow policyâ€™s time distribution and is achievable at four steps in Hopper (A) and one step in Square (B, C).
            </div>
        </div>
        <h4>Flow Matching's Time Distribution</h4>
        <p>The effectiveness of ReinFlow is not affected by altering the time sampling distribution of the pre-trained flow matching policy. However, beta distribution is slightly stronger when fine-tuning at one denoising step, as in the case in the figure above (C).</p>
        <h4>Noise Network Inputs</h4>
        <p>The inputs of the noise injection network affect the performance of fine-tuned flow policies. As shown in the figure below, conditioning on both observations and the time often yields a higher success rate, as this approach allows the noise network to learn how to create more diverse actions by altering the noise intensity at different denoising steps.</p>
        <div class="figure-container">
            <div class="subfigure-row">
                <div class="subfigure-2">
                    <img src="figs/jpgs/noise_parameterization_ant-page1.jpg" alt="Noise Input's Effect in Ant-v0">
                    <div class="subfigure-caption">(A) Noise Input's Effect in Ant-v0</div>
                </div>
                <div class="subfigure-2">
                    <img src="figs/jpgs/kitchen_kitchen-partial-v0-sigma_s_t_SuccessRate-page1.jpg" alt="Noise Condition's Effect in Kitchen-partial">
                    <div class="subfigure-caption">(B) Noise Condition's Effect in Kitchen-partial</div>
                </div>
            </div>
            <div class="caption">
                Conditioning on state and time yields a higher success rate than only conditioning on states.
            </div>
        </div>
        <h4>Noise Level and Exploration</h4>
        <p>We find that the magnitude of the injected noise is the most significant factor affecting ReinFlow's performance. In the figure below (A), we add constant noise in 'Ant-v0' with different standard deviation levels. When the noise is minimal (std=0.01), the policy's reward remains around the pre-trained baseline, showing limited exploration. After enlarging the noise to a certain threshold, we find that the agent quickly explores new policies that are three times stronger than the base policy, after which ReinFlow becomes less sensitive to changes in noise level. We also discover that tuning down the noise is beneficial for visuomotor policies, complex tasks, long denoising chains, and policies with poor pre-training performance.</p>
        <h4>Regularization and Exploration</h4>
        <p>ReinFlow admits various regularizations, and we find that it is generally unnecessary to constrain the policy with a Wasserstein-2 regularizer while adding entropy regularization, which is beneficial in locomotion tasks. In the figure below (B), we train ReinFlow in a "Humanoid-v3" environment. By gradually tuning down the $W_2$ regularization coefficient $\beta$, the fine-tuned policy leaves the behavior cloning (BC) baseline and discovers significantly more robust actions, approaching the curve trained with entropy regularization of intensity $\alpha=0.03$. This finding also partially explains why the offline RL method FQL, which adopts $W_2$ distillation loss to restrict online exploration, performs worse than our pure online RL method ReinFlow.</p>
        <div class="figure-container">
            <div class="subfigure-row">
                <div class="subfigure-2">
                    <img src="figs/jpgs/constant_noise_level-page1.jpg" alt="Noise Level Affects Exploration in Ant-v0">
                    <div class="subfigure-caption">(A) Noise Level Affects Exploration in Ant-v0</div>
                </div>
                <div class="subfigure-2">
                    <img src="figs/jpgs/gym-state_humanoid-regularize-compare_AverageEpisodeReward-page1.jpg" alt="Regularization Affects ReinFlow in Humanoid-v3">
                    <div class="subfigure-caption">(B) Regularization Affects ReinFlow in Humanoid-v3</div>
                </div>
            </div>
            <div class="caption">
                Noise level and regularization's effect. (A) demonstrates constant noise with different standard deviation affects ReinFlow's exploration. (B) shows how entropy regularization with coefficient $\alpha$ and $W_2$ regularization with different coefficients $\beta$ influences ReinFlow.
            </div>
        </div>
    </section>
</body>
</html>